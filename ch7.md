### 앙상블 학습과 랜덤 포레스트
- 앙상블 학습 : 여러 개의 모델을 훈련하고 그 예측들을 결합하여 최종 예측을 도출하는 방식
- 각 분류기가 약한 학습기라도 약한 학습기가 충분히 많고 다양하다면 강한 학습기가 될 수 있음
- 큰 수의 법칙 : 앞면이 51%, 뒷면이 49%가 나오는 동전이 있다고 하면 1000번을 던진 후 앞면이 다수가 될 확률은 75%에 가까움

![7-2](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-3.png)
- 51% 정확도를 가진 1000개의 분류기로 앙상블 모델을 구축한다면 75%의 정확도를 기대할 수 있음
- 단, 모든 분류기가 완벽하게 독립적이고 오차에 상관관계가 없어야 함
- 다양한 분류기를 얻는 대표적인 한 가지 방법으로 각기 다른 알고리즘으로 학습시키는 방법이 있음

## 7.1 투표 기반 분류기
![7-2](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-2.png)
- 직접 투표 : 다수결 투표
- 간접 투표 : 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측
- 간접 투표가 직접 투표보다 성능이 높음

## 7.2 배깅과 페이스팅
![7-4](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-4.png)
- 다양한 분류기를 얻는 다른 방법으로 훈련 세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습 시키는 것
- 배깅(bagging) : 중복을 허용하여 샘플링 하는 방식 (bootstrap aggregating)
- 페이스팅(pasting) : 중복을 허용하지 않고 샘플링 하는 방식
- 모든 예측기가 훈련을 마치면 앙상블은 분류일 때는 통계적 최빈값(가장 많은 예측 결과 like 직접 투표 분류기)을, 회귀에 대해서는 평균을 계산해 결과를 냄
- 앙상블 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산이 줄어듦
- 편향 : 베깅 < 페이스팅 (페이스팅이 더 우수)
- 분산 : 베깅 > 페이스팅 (베깅이 더 우수)
- 전반적으로 베깅이 더 나은 모델을 만듬
- OOB 평가 : 베깅에서 out-of-bag로 훈련에 사용되지 않은 나머지 샘플을 각 예측기가 평가해 그 평균으로 얻은 평가 점수

![pv](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/pv.png)   
[[이미지 출처]](https://datacookbook.kr/48)
![7-5](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-5.png)
- bias : 편향, variance : 분산
- 학습이나 예측을 병렬로 수행할 수 있어서 인기가 높음

## 7.3 랜덤 패치와 랜덤 서브스페이스
- 샘플 뿐 아니라 특성 샘플링도 지원
- 랜덤 패치 방식 : 특성과 샘플을 모두 샘플링
- 랜덤 서브스페이스 방식 : 특성만 샘플링
## 7.4 랜덤 포레스트
- 베깅 방법(또는 페이스팅)을 적용한 결정 트리 앙상블
- 엑스트라 트리 : 트리를 더욱 랜덤하게 만들기 위해 최적의 임곗값을 찾는 대신 후보 특성을 사용해 랜덤으로 분할한 다음 그중 최상의 분할을 선택하는 방식, 편향이 늘어나는 대신 분산이 낮아짐
- 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정 가능

![7-6](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-6.png)
## 7.5 부스팅
- 약한 학습기 여러 개를 연결하여 강한 학습기를 만드는 앙상블 방법
- 앞의 모델을 보완해 나가면서 일련의 예측기를 학습 시키는 아이디어

![7-7](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-7.png)
![adb](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/adaboost.png)   
[[이미지 출처]](https://bommbom.tistory.com/entry/Boosting-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-Adaboost-%EB%8F%99%EC%9E%91-%EC%9B%90%EB%A6%AC)
- AdaBoost : 이전 예측기를 보완하는 새로운 예측기를 만드는 방법은 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것
- 첫 번째 약한 학습기가 첫 번째 분류 기준(D1)으로 +와 -를 분류
- 잘못 분류된 데이터에 대해 가중치를 부여(두 번째 그림에서 커진 + 표시)
- 두 번째 약한 학습기가 두 번째 분류 기준(D2)으로 +와 -를 다시 분류
- 잘못 분류된 데이터에 대해 가중치를 부여(세 번째 그림에서 커진 - 표시)
- 세 번째 약한 학습기가 세 번째 분류 기준(D3)으로 +와 -를 다시 분류해서 오류 데이터를 찾음
- 마지막으로 분류기들을 결합하여 최종 예측 수행(네 번째 그림)

![7-9](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-9.png)
![gb](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/gb.png)   
[[이미지 출처]](https://beavekim23.tistory.com/3)
- 그레이디언트 부스팅 : 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습 시킴, 각 트리의 예측값을 합쳐서 최종 예측 수행
- 히스토그램 기반 그레이디언트 부스팅 : 특성의 구간을 분할하여 평가해야 하는 임곗값의 수를 크게 줄여 성능을 향상시킴, 규제처럼 작동해서 정밀도 손실을 유발하므로 데이터 셋에 따라서 과대 적합을 줄이는 데 도움이 될 수도 있고 과소 적합을 유발할 수도 있음
<details>
<summary>접기/펼치기</summary>

## Chat GPT 설명
히스토그램 기반 그레디언트 부스팅(Histogram-based Gradient Boosting)은 머신 러닝에서 널리 사용되는 그레디언트 부스팅(Gradient Boosting) 알고리즘을 더욱 효율적으로 구현하기 위한 방법 중 하나입니다. 이 방법은 특히 대용량 데이터셋에서도 빠르게 동작할 수 있기 때문에 많은 관심을 받고 있습니다.

다양한 방식으로 그레디언트 부스팅을 최적화하지만, 히스토그램 기반 접근법은 특히 결정 트리 학습 시 데이터 분할 작업을 더 효율적으로 처리하는 데 주안점을 둡니다. 이 방식은 범주형 및 연속형 피처를 불연속 구간으로 변환하여 처리하는데, 이를 통해 메모리 사용량과 계산 복잡도를 크게 줄일 수 있습니다.

다음은 히스토그램 기반 그레디언트 부스팅의 동작 원리를 단계별로 설명한 것입니다:

### 데이터 준비
- **데이터**: 우리가 사용할 데이터는 여러 피처를 가진 N개의 샘플로 구성됩니다.
- **레벨(bin) 설정**: 각 피처를 고정된 수의 구간(bin)으로 나눕니다. 예를 들어, 연속형 피처를 10개의 구간으로 나눈다고 가정합시다.

### 데이터 변환
- **히스토그램 생성**: 각 피처에 대해 이를 설정한 구간(bin)으로 변환합니다. 이 과정을 통해 데이터의 종류를 크게 줄일 수 있습니다.
  - 예시: 어떤 피처가 [1.1, 2.3, 2.9, 4.4, 5.8]과 같은 값을 가질 때, 이를 5개의 구간으로 나누면 [1, 2, 2, 3, 4]와 같이 변환할 수 있습니다.

### 히스토그램 기반 분할
1. **초기 조건**: Boosting의 기초 아이디어는 초기 모델을 매우 단순하게 시작하고, 각 단계에서 이전 단계의 잔차(residual)에 맞춰 새로운 모델을 추가하며 개선하는 것입니다.
  
2. **잔차 계산**: 초기 모델을 통해서 얻은 예측값과 실제 값의 차이(잔차)를 계산하여, 그 잔차를 최소화하는 방향으로 새로운 모델(트리)을 학습합니다.

3. **분할 후보 선택**: 각 노드에서 데이터를 분할할 때 히스토그램을 사용하여 분할 후보들을 선택합니다. 이는 연속적인 피처 값을 일일이 비교하는 대신, 히스토그램 간의 비교로 대체되므로 상당히 빠릅니다.

4. **정보 획득량 계산**: 각 구간(bin)에서 정보 획득량(information gain)을 계산하여 최적의 분할 지점을 찾습니다.

5. **트리 생성 및 잔차 업데이트**: 최적의 분할 지점에서 데이터를 나누고, 이에 따라 새로운 잔차를 계산합니다. 이 과정을 반복하여 트리를 성장시킵니다.

6. **학습 반복**: 여러 번의 반복을 통해 모델을 계속 발전시킵니다. 각각의 반복에서는 이전 모델의 잔차를 줄이기 위해 새로운 약한 모델(트리)을 추가합니다.

### 예시
가상의 예시를 통해 히스토그램 기반 그레디언트 부스팅을 적용해 보겠습니다:

#### 데이터셋
- 피처 X1: [0.5, 1.5, 1.7, 3.0, 3.2, 4.8, 5.1]
- 피처 X2: [10, 20, 15, 30, 25, 35, 40]
- 타겟 Y: [100, 150, 130, 190, 180, 200, 210]

1. **히스토그램 생성**:
   - X1: [0.5, 1.5, 1.7, 3.0, 3.2, 4.8, 5.1] → 구간(bin) 5개로 나누면 [1, 2, 2, 4, 4, 5, 5]
   - X2: [10, 20, 15, 30, 25, 35, 40] → 구간(bin) 5개로 나누면 [1, 2, 2, 3, 3, 4, 5]

2. **초기 조건**: 초기 모델은 평균값인 165로 설정합니다.

3. **잔차 계산**: 초기 모델의 예측값과 실제 값의 차이를 계산하여 잔차를 구합니다.
   - 잔차: [ -65, -15, -35, 25, 15, 35, 45]

4. **분할 후보 선택 및 정보 획득량 계산**: 히스토그램에서 각 구간(bin)별로 잔차를 분할하여 정보를 획득할 수 있는 최적의 지점을 찾습니다.

5. **트리 생성**: 최적의 분할 지점을 찾아 데이터를 두 개의 노드로 나눕니다. 이 과정을 반복하여 트리를 완성합니다.

6. **학습 반복**: 동일 과정을 여러 번 반복하여 점차 모델을 개선합니다.

히스토그램 기반 그레디언트 부스팅은 대량의 데이터셋에서도 효율적으로 학습할 수 있는 강력한 도구입니다. 이 방법은 연산 시간을 줄이고, 메모리 사용을 효율화하며, 계산 복잡도를 줄이기 때문에 매우 유용합니다. Sci-kit Learn의 `HistGradientBoostingClassifier`와 `HistGradientBoostingRegressor`, 그리고 마이크로소프트의 LightGBM 등 여러 라이브러리에서 이 방식을 지원합니다.
</details>

## 7.6 스태킹
- 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수 대신 취합하는 모델을 훈련 시키는 아이디어

![7-12](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-12.png)
- 성능을 조금 더 끌어올릴 수 있지만 훈련 시간과 시스템 복잡성 측면에서 비용이 증가
