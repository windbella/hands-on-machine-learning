### 앙상블 학습과 랜덤 포레스트
- 앙상블 학습 : 여러 개의 모델을 훈련하고 그 예측들을 결합하여 최종 예측을 도출하는 방식
- 각 분류기가 약한 학습기라도 약한 학습기가 충분히 많고 다양하다면 강한 학습기가 될 수 있음
- 큰 수의 법칙 : 앞면이 51%, 뒷면이 49%가 나오는 동전이 있다고 하면 1000번을 던진 후 앞면이 다수가 될 확률은 75%에 가까움

![7-2](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-3.png)
- 51% 정확도를 가진 1000개의 분류기로 앙상블 모델을 구축한다면 75%의 정확도를 기대할 수 있음
- 단, 모든 분류기가 완벽하게 독립적이고 오차에 상관관계가 없어야 함
- 다양한 분류기를 얻는 대표적인 한 가지 방법으로 각기 다른 알고리즘으로 학습시키는 방법이 있음

## 7.1 투표 기반 분류기
![7-2](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-2.png)
- 직접 투표 : 다수결 투표
- 간접 투표 : 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측
- 간접 투표가 직접 투표보다 성능이 높음

## 7.2 배깅과 페이스팅
![7-4](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-2.png)
- 다양한 분류기를 얻는 다른 방법으로 훈련 세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습 시키는 것
- 배깅(bagging) : 중복을 허용하여 샘플링 하는 방식 (bootstrap aggregating)
- 페이스팅(pasting) : 중복을 허용하지 않고 샘플링 하는 방식
- 모든 예측기가 훈련을 마치면 앙상블은 분류일 때는 통계적 최빈값(가장 많은 예측 결과 like 직접 투표 분류기)을, 회귀에 대해서는 평균을 계산해 결과를 냄
- 앙상블 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산이 줄어듦
- 편향 : 베깅 < 페이스팅 (페이스팅이 더 우수)
- 분산 : 베깅 > 페이스팅 (베깅이 더 우수)
- 전반적으로 베깅이 더 나은 모델을 만듬
- OOB 평가 : out-of-bag로 훈련에 사용되지 않은 나머지 샘플을 각 예측기가 평가해 그 평균으로 얻은 평가 점수

![7-4](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/pv.png)
![7-4](https://github.com/windbella/hands-on-machine-learning/blob/main/ch7/7-5.png)
- bias : 편향, variance : 분산
- 학습이나 예측을 병렬로 수행할 수 있어서 인기가 높음

## 7.3 랜덤 패치와 랜덤 서브스페이스
## 7.4 랜덤 포레스트
## 7.5 부스팅
## 7.6 스태킹
