### 심층 신경만 훈련

## 그레이디언트 소실과 폭주 문제
- 그레이디언트 소실
  - 하위 층으로 진행될수록 그레이디언트가 점점 작아지는 현상
- 그레이디언트 폭주
  - 하위 층으로 진행될수록 그레이디언트가 점점 커져서 발산하는 현상
- 원인
  - 시그모이드 활성 함수를 사용하고 평균이 0이고 표준 편차가 1인 정규 분포로 초기화했을 때 출력의 분산이 입력의 분산보다 큼
  - 신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져 높은 층에서는 활성화 함수가 0이나 1로 수렴
  - 입력이 커지면 0이나 1로 수렴하고 기울기가 0에 가까워져 역전파가 될 때 사실상 하위 신경망으로 전달할 그레이디언트가 거의 없고 아래쪽 층에는 아무것도 도달하지 않게 됨   
![함수의 수렴](https://velog.velcdn.com/images/kyungmin1029/post/def9ea6a-4ba6-4b14-9850-2e6e9c166355/image.png)

## 해결 방법
- 이론
  - 예측 시 정방향, 역전파 시 역방향, 양방향 신호가 적절히 흘러야 함
  - 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 함
  - 역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야 함
- 초기화
  - 각 층의 연결 가중치를 적절히 초기화 하면 문제를 줄일 수 있음
  - 글럿 초기화 (시그모이드 활성 함수를 사용할 때)
![글럿 초기화](https://velog.velcdn.com/images/kyungmin1029/post/3a137164-be17-4271-81b0-bfc9f31e8a95/image.png)
  - 활성화 함수별 초기화 전략   
![초기화 전략](https://velog.velcdn.com/images/kyungmin1029/post/9d13fc2e-b393-41b8-89ba-810942ba5e7c/image.png)
- 활성화 함수
  - 예전에는 생물학적 뉴런의 방식과 비슷한 시그모이드 활성 함수가 최선의 선택으로 생각됨
  - 다른 활성화 함수가 심층 신경망에서 훨씬 더 잘 작동한다는 사실이 밝혀짐 (특히 ReLU, Rectified Linear Unit)
  - ReLU 함수도 완벽하지 않고, 죽은 ReLU 등 여러 문제가 있기 때문에 개선이나 더 좋은 함수를 찾기 위한 여러 시도가 있음
  - 죽은 ReLU : 일부 뉴런이 훈련하는 동안 0이외의 값을 출력하지 않는 현상
- 배치 정규화
  - 초기화 및 활성화 함수 선택을 적절히 하면 훈련 초기 단계에 문제를 줄일 수 있지만 훈련이 진행되다 보면 다시 문제가 발생할 수 있음
  - 각 층에 배치 정규화 층을 둬서 문제를 해결할 수 있음
  ```
  model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(300, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(10, activation="softmax")
  ])
  ```
  - 배치 정규화는 규제와 같은 역할을 하여 다른 규제 기법의 필요성을 줄여줌
  - 그러나 배치 정규화는 모델의 복잡도를 키워 무겁고 느려짐
- 그레이디언트 클리핑
  - 역전파 될 때 특정 임곗값을 넘어서지 못하도록 그레이디언트를 잘라내는 것
  - 일반적으로 배치 정규화를 사용하기 까다로운 순환 신경망에서 사용
