### 심층 신경만 훈련

## 그레이디언트 소실과 폭주 문제
- 그레이디언트 소실
  - 하위 층으로 진행될수록 그레이디언트가 점점 작아지는 현상
- 그레이디언트 폭주
  - 하위 층으로 진행될수록 그레이디언트가 점점 커져서 발산하는 현상
- 원인
  - 시그모이드 활성 함수를 사용하고 평균이 0이고 표준 편차가 1인 정규 분포로 초기화했을 때 출력의 분산이 입력의 분산보다 큼
  - 신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져 높은 층에서는 활성화 함수가 0이나 1로 수렴
  - 입력이 커지면 0이나 1로 수렴하고 기울기가 0에 가까워져 역전파가 될 때 사실상 하위 신경망으로 전달할 그레이디언트가 거의 없고 아래쪽 층에는 아무것도 도달하지 않게 됨   
![함수의 수렴](https://velog.velcdn.com/images/kyungmin1029/post/def9ea6a-4ba6-4b14-9850-2e6e9c166355/image.png)

## 해결 방법
- 이론
  - 예측 시 정방향, 역전파 시 역방향, 양방향 신호가 적절히 흘러야 함
  - 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 함
  - 역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야 함
- 초기화
  - 각 층의 연결 가중치를 적절히 초기화 하면 문제를 줄일 수 있음
  - 글럿 초기화 (시그모이드 활성 함수를 사용할 때)
![글럿 초기화](https://velog.velcdn.com/images/kyungmin1029/post/3a137164-be17-4271-81b0-bfc9f31e8a95/image.png)
  - 활성화 함수별 초기화 전략   
![초기화 전략](https://velog.velcdn.com/images/kyungmin1029/post/9d13fc2e-b393-41b8-89ba-810942ba5e7c/image.png)
- 활성화 함수
  - 예전에는 생물학적 뉴런의 방식과 비슷한 시그모이드 활성 함수가 최선의 선택으로 생각됨
  - 다른 활성화 함수가 심층 신경망에서 훨씬 더 잘 작동한다는 사실이 밝혀짐 (특히 ReLU, Rectified Linear Unit)
  - ReLU 함수도 완벽하지 않고, 죽은 ReLU 등 여러 문제가 있기 때문에 개선이나 더 좋은 함수를 찾기 위한 여러 시도가 있음
  - 죽은 ReLU : 일부 뉴런이 훈련하는 동안 0이외의 값을 출력하지 않는 현상
- 배치 정규화
  - 초기화 및 활성화 함수 선택을 적절히 하면 훈련 초기 단계에 문제를 줄일 수 있지만 훈련이 진행되다 보면 다시 문제가 발생할 수 있음
  - 각 층에 배치 정규화 층을 둬서 문제를 해결할 수 있음
  ```
  model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(300, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(10, activation="softmax")
  ])
  ```
  - 배치 정규화는 규제와 같은 역할을 하여 다른 규제 기법의 필요성을 줄여줌
  - 그러나 배치 정규화는 모델의 복잡도를 키워 무겁고 느려짐
- 그레이디언트 클리핑
  - 역전파 될 때 특정 임곗값을 넘어서지 못하도록 그레이디언트를 잘라내는 것
  - 일반적으로 배치 정규화를 사용하기 까다로운 순환 신경망에서 사용
 
## 사전 훈련도니 층 재사용하기
- 전이 학습
  - 해결하려는 문제와 유사한 유형의 문제를 처리한 신경망이 있는지 찾음
  - 최상위 층을 제외한 다른 층들을 재사용하여 훈련 속도를 크게 향상시킬 수 있음
  - 속도뿐 아니라 필요한 훈련 데이터도 크게 줄여줌
![재사용](https://velog.velcdn.com/images/kyungmin1029/post/1c75d11c-abe9-4ac6-9249-59b1723c04a7/image.png)
- 비지도 사전 훈련
  - 레이블 된 훈련 데이터는 적고, 레이블 되지 않은 훈련 데이터를 많이 모을 수 있는 상황이 흔히 발생함
  - 레이블 되지 않은 데이터 오토인코더나 GAN을 이용해 비지도 훈련을 실행
  - 비지도 훈련한 모델의 하위 층을 재사용하여 지도 학습을 실행해 최종 네트워크를 세밀하게 튜닝함
![비지도 사전 훈련](https://velog.velcdn.com/images/kyungmin1029/post/dff201c5-f432-4b06-98c7-f8d0e21987a2/image.png)
- 보조 작업에서 사전 훈련
  - 쉽게 얻을 수 있는 데이터를 활용한 보조 작업을 먼저 훈련하고, 훈련된 모델의 하위 층을 재사용해 실제 훈련을 진행
  - 예로 각 사람의 얼굴 사진을 수백 개씩 모으긴 어렵기 때문에 랜덤으로 많은 인물의 이미지를 수집해 두 개의 다른 이미지가 같은 사람 것인지 감지하는 신경망을 훈련
  - 얼굴의 특성을 잘 감지하도록 훈련된 이 모델을 이용해서 얼굴을 인식하는 실제 모델을 적은 양의 훈련 데이터로도 훈련이 가능
  - 자연어 처리에서도 많이 활용 됨 (랜덤한 말뭉치의 중간을 임의로 제거하고 누락된 단어를 예측하는 모델을 훈련 시키면 다른 작업에서도 유용하게 사용됨)
