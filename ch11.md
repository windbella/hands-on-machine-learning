### 심층 신경만 훈련

## 그레이디언트 소실과 폭주 문제
- 그레이디언트 소실
  - 하위 층으로 진행될수록 그레이디언트가 점점 작아지는 현상
- 그레이디언트 폭주
  - 하위 층으로 진행될수록 그레이디언트가 점점 커져서 발산하는 현상
- 원인
  - 시그모이드 활성 함수를 사용하고 평균이 0이고 표준 편차가 1인 정규 분포로 초기화했을 때 출력의 분산이 입력의 분산보다 큼
  - 신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져 높은 층에서는 활성화 함수가 0이나 1로 수렴
  - 입력이 커지면 0이나 1로 수렴하고 기울기가 0에 가까워져 역전파가 될 때 사실상 하위 신경망으로 전달할 그레이디언트가 거의 없고 아래쪽 층에는 아무것도 도달하지 않게 됨

![함수의 수렴](https://velog.velcdn.com/images/kyungmin1029/post/def9ea6a-4ba6-4b14-9850-2e6e9c166355/image.png)

## 해결 방법
- 초기화
- 활성화 함수
- 배치 정규화
