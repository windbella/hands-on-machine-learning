## 결정 트리
### 6.1 결정 트리 학습과 시각화
![6-1](https://github.com/windbella/hands-on-machine-learning/blob/main/ch6/6-1.png)
- 결정 트리는 시각화가 가능함
### 6.2. 예측
- 지니 불순도 : 한 노드의 샘플이 같은 클래스에 속해 있다면 gini = 0
- 결정 트리는 직관적이고 결정 방식을 이해하기 쉬움. 이러한 모델을 화이트박스 모델이라고 함 (반대는 블랙박스 모델)
### 6.3 클래스 확률 추정
- 노드의 훈련 샘플 비율을 이용해서 확률 출력 가능
### 6.4 CART 훈련 알고리즘
- 특성 k에 대해서 분할한 노드가 초대한 순수하도록 (gini = 0에 가까운) 나누는 임곗값 t를 찾는 알고리즘
- 최대 깊이가 되거나 불순도를 줄이는 분할을 찾을 수 없을 때 멈춤
- 탐욕 알고리즘, 최적의 트리를 찾는 것은 NP-완전 문제로 매우 작은 세트에 적용하기도 어려움
### 6.7 규제 매개변수
- 결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없어서 과대 적합되기 쉬움
- 결정 트리는 훈련되기 전에 파라미터 수가 결정되지 않기 때문에 비파라미터 모델이라고 함
### 6.8 회귀
![6-6](https://github.com/windbella/hands-on-machine-learning/blob/main/ch6/6-6.png)
- 클래스를 예측하는 대신 어떠한 값을 예측하게 하면 회귀도 가능
### 6.9 축 방향에 대한 민감성
- 축에 수직인 계단 모양의 결정 경계를 만들기 때문에 축에 민감함
- PCA(주성분 분석) 변환 변환을 이용해 특성 간의 상관관계를 줄여 데이터를 회전 시켜 결정 트리를 더 쉽게 만들 수 있음
### 6.10 결정 트리의 분산 문제
- 결정 트리의 주요 문제는 분산이 상당히 크다는 것
- 하이퍼파라미터나 데이터를 조금만 바꿔도 매우 다른 모델이 생성 될 수 있음
- 사이런킷에서 사용하는 훈련 알고리즘은 확률적이므로 동일한 결정 트리를 재훈련해도 다른 결과가 나올 수 있음
- 여러 결정 트리의 예측을 평균하면 분산을 크게 줄일 수 있음 -> 결정 트리의 앙상블, 랜덤 포레스트
