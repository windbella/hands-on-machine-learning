### 케라스를 사용한 인공 신경망 소개
## 인공뉴런
![생물학적 뉴런](https://velog.velcdn.com/images/kyungmin1029/post/3afa72bc-ccd2-40f5-a2bf-c52f509ce7e4/image.png)
- 생물학적 뉴런   

![인공 뉴런](https://velog.velcdn.com/images/kyungmin1029/post/4f9a3043-52a6-4ac0-a9da-0ff5655c64ba/image.png)
- 인공 뉴런 예시
  - 두 개 이상의 신호가 입력되면 활성화
 
## 퍼셉트론
![퍼셉트론](https://velog.velcdn.com/images/kyungmin1029/post/4c4c42ca-49c2-4a95-ab9d-b170645a38d5/image.png)
- 입력에 선형 함수를 적용해 가중치를 적용하고 계단 함수를 통과시켜 출력을 제어

![헤비사이드](https://velog.velcdn.com/images/kyungmin1029/post/c219998d-ed7a-489c-acb6-468201c72844/image.png)
- 헤비사이드 계단 함수가 퍼셉트론에서 계단함수로 많이 이용 됨

## 퍼셉트론의 훈련
- 헤브의 규칙 : 생물학적 뉴런이 다른 뉴런을 활성화 시킬 때 두 뉴런의 연결이 더 강해진다는 제안
- 퍼셉트론에 한번에 한 개의 샘플이 주입되면 각 샘플에 대해 예측이 만들어짐
- 잘못된 예측을 하는 모든 출력 뉴런에 대해 올바른 예측을 만들 수 있도록 입력에 연결된 가중치를 강화
- 이는 확률적 경사 하강법과 유사

## 다층 퍼셉트론 (MLP)
- 단층 퍼셉트론은 XOR과 같은 일부 간단한 문제를 풀 수 없음 (다른 선형 분류기도 마찬가지)
- 퍼셉트론을 여러개 쌓아올리면 일부 제약을 줄 일 수 있음

![XOR](https://velog.velcdn.com/images/kyungmin1029/post/76c79d04-32a0-4025-97cf-4ef338daa886/image.png)
- XOR 예시

## 역전파
- 여러 층을 쌓는 건 좋았지만 훈련하는 방법을 찾기가 어려웠음
- 후진 모드 자동 미분 (reverse-mode autodiff)
  - 정방향으로 진행하면서 중간 계산값을 모두 저장 (정방향 계산)
  - 알고리즘이 네트워크 출력 오차를 측정
  - 각 출력의 오차와 출력 층의 각 연결이 이 오차에 얼마나 기여했는지 계산
  - 연쇄법칙을 사용하여 이전 층의 연결 가중치가 이 오차에 얼마나 기여했는지 측정
  - 입력 층에 도달할 때까지 반복 (역방향 계산)
  - 위와 같은 방법으로 모든 연결 가중치와 편향에 대한 오차 그레이디언트를 측정
  - 마지막으로 경사 하상법을 수행하여 네트워크에 있는 모든 연결 가중치를 수정 (경사 하강법 단계)
 
![도함수 비교](https://velog.velcdn.com/images/kyungmin1029/post/8cc0a781-14b3-4928-9659-df68e7172971/image.png)
- 위 알고리즘을 잘 동작시키고자 하면 수평선만 있는 계단함수는 부적합함
- 시그모이드, tanh (하이퍼볼릭 탄젠트), ReLU 함수 등 비선형 함수로 변경함으로써 그레이디언트가 잘 정의되게 해 원활하게 경사 하강법이 진행되게 할 수 있음
- 선형 변환을 여러 개 연결해도 결국 선형 변환이지만, 중간비 비선형을 추가함으로써 복잡한 문제를 풀 수 있게 함

## 케라스로 다층 퍼셉트론 구현하기
- https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb
- !pip uninstall tensorflow
- !pip install tensorflow==2.8
- 낮은 버전의 tensorflow가 설치되어 있는 경우가 있어 요구하는 버전으로 재설치

- 시퀀셜 API
```
model = tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(1)
])
```
- 함수형 API
```
normalization_layer = tf.keras.layers.Normalization()
hidden_layer1 = tf.keras.layers.Dense(30, activation="relu")
hidden_layer2 = tf.keras.layers.Dense(30, activation="relu")
concat_layer = tf.keras.layers.Concatenate()
output_layer = tf.keras.layers.Dense(1)

input_ = tf.keras.layers.Input(shape=X_train.shape[1:])
normalized = normalization_layer(input_)
hidden1 = hidden_layer1(normalized)
hidden2 = hidden_layer2(hidden1)
concat = concat_layer([normalized, hidden2])
output = output_layer(concat)

model = tf.keras.Model(inputs=[input_], outputs=[output])
```
- 서브클래싱 API
```
class WideAndDeepModel(tf.keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs)  # 모델 이름을 지정하는 데 필요합니다
        self.norm_layer_wide = tf.keras.layers.Normalization()
        self.norm_layer_deep = tf.keras.layers.Normalization()
        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)
        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)
        self.main_output = tf.keras.layers.Dense(1)
        self.aux_output = tf.keras.layers.Dense(1)

    def call(self, inputs):
        input_wide, input_deep = inputs
        norm_wide = self.norm_layer_wide(input_wide)
        norm_deep = self.norm_layer_deep(input_deep)
        hidden1 = self.hidden1(norm_deep)
        hidden2 = self.hidden2(hidden1)
        concat = tf.keras.layers.concatenate([norm_wide, hidden2])
        output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return output, aux_output

tf.random.set_seed(42)  # 추가 코드 - 재현성을 위한 것
model = WideAndDeepModel(30, activation="relu", name="my_cool_model")
```
- 모델 저장 및 복원
  - 모델을 저장하고 불러올 수 있느 기능을 이용해 체크포인트를 저장해 효율적으로 훈련 가능
- 콜백 사용하기
  - on_train_begin(), on_train_end() 등 다양한 이벤트를 지원하여 훈련 중 여러가지 상황을 제어 가능
  - 예로 일정 간격으로 체크포인트를 저장하거, 조기 종료를 시킬 수 있음
- 신경망 하이퍼파라미터 튜닝
  - 신경망의 유연함은 장점이지만 조정할 하이퍼파라미터가 많음
  - GridSearch(직접 입력), RandomizeSearch(랜덤 적용) 등을 활용하여 하이퍼파라미터를 튜닝 가능
