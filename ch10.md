### 케라스를 사용한 인공 신경망 소개
## 인공뉴런
![생물학적 뉴런](https://velog.velcdn.com/images/kyungmin1029/post/3afa72bc-ccd2-40f5-a2bf-c52f509ce7e4/image.png)
- 생물학적 뉴런   

![인공 뉴런](https://velog.velcdn.com/images/kyungmin1029/post/4f9a3043-52a6-4ac0-a9da-0ff5655c64ba/image.png)
- 인공 뉴런 예시
  - 두 개 이상의 신호가 입력되면 활성화
 
## 퍼셉트론
![퍼셉트론](https://velog.velcdn.com/images/kyungmin1029/post/4c4c42ca-49c2-4a95-ab9d-b170645a38d5/image.png)
- 입력에 선형 함수를 적용해 가중치를 적용하고 계단 함수를 통과시켜 출력을 제어

![헤비사이드](https://velog.velcdn.com/images/kyungmin1029/post/c219998d-ed7a-489c-acb6-468201c72844/image.png)
- 헤비사이드 계단 함수가 퍼셉트론에서 계단함수로 많이 이용 됨

## 퍼셉트론의 훈련
- 헤브의 규칙 : 생물학적 뉴런이 다른 뉴런을 활성화 시킬 때 두 뉴런의 연결이 더 강해진다는 제안
- 퍼셉트론에 한번에 한 개의 샘플이 주입되면 각 샘플에 대해 예측이 만들어짐
- 잘못된 예측을 하는 모든 출력 뉴런에 대해 올바른 예측을 만들 수 있도록 입력에 연결된 가중치를 강화
- 이는 확률적 경사 하강법과 유사

## 다층 퍼셉트론 (MLP)
- 단층 퍼셉트론은 XOR과 같은 일부 간단한 문제를 풀 수 없음 (다른 선형 분류기도 마찬가지)
- 퍼셉트론을 여러개 쌓아올리면 일부 제약을 줄 일 수 있음

![XOR](https://velog.velcdn.com/images/kyungmin1029/post/76c79d04-32a0-4025-97cf-4ef338daa886/image.png)
- XOR 예시

## 역전파
- 여러 층을 쌓는 건 좋았지만 훈련하는 방법을 찾기가 어려웠음
- 후진 모드 자동 미분 (reverse-mode autodiff)
  - 정방향으로 진행하면서 중간 계산값을 모두 저장 (정방향 계산)
  - 알고리즘이 네트워크 출력 오차를 측정
  - 각 출력의 오차와 출력 층의 각 연결이 이 오차에 얼마나 기여했는지 계산
  - 연쇄법칙을 사용하여 이전 층의 연결 가중치가 이 오차에 얼마나 기여했는지 측정
  - 입력 층에 도달할 때까지 반복 (역방향 계산)
  - 위와 같은 방법으로 모든 연결 가중치와 편향에 대한 오차 그레이디언트를 측정
  - 마지막으로 경사 하상법을 수행하여 네트워크에 있는 모든 연결 가중치를 수정 (경사 하강법 단계)
 
![도함수 비교](https://velog.velcdn.com/images/kyungmin1029/post/8cc0a781-14b3-4928-9659-df68e7172971/image.png)
- 위 알고리즘을 잘 동작시키고자 하면 수평선만 있는 계단함수는 부적합함
- 시그모이드, tanh (하이퍼볼릭 탄젠트), ReLU 함수 등 비선형 함수로 변경함으로써 그레이디언트가 잘 정의되게 해 원활하게 경사 하강법이 진행되게 할 수 있음
- 선형 변환을 여러 개 연결해도 결국 선형 변환이지만, 중간비 비선형을 추가함으로써 복잡한 문제를 풀 수 있게 함
